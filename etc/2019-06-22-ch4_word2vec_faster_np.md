---
layout: post
title: "[NLP] ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹2 - Ch4 : word2vec ê°œì„ " 
category: nlp
tags: dl nlp word2vec ìì—°ì–´ì²˜ë¦¬ Embedding ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹2
comments: true
img: nlp3.jpg
---





# 4. word2vec ì†ë„ ê°œì„ Â 

---

`ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹2`ì˜ **4ì¥ì—ì„œëŠ”** ì§€ì§€ë‚œì£¼ì— ë´¤ë˜ [ch3. word2vec](<https://taeu.github.io/nlp/ch3_word2vec/>) ì˜ ì†ë„ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ í¬ê²Œ 2ê°€ì§€ë¥¼ ì‚´í´ë³¼ ì˜ˆì •ì´ë‹¤. ìš°ì„  ì½”ë“œë¥¼ ëŒë¦¬ê¸°ì— ì•ì„œ GPU ì—°ì‚°ì„ ìœ„í•´ ì•„ë˜ì™€ ê°™ì€ ì½”ë“œë¥¼ ì‹¤í–‰í•œë‹¤. ì±…ì— ë‚˜ì™€ìˆëŠ” ì½”ë“œì—ì„œ `np.add.at = np.scatter_add`ë¶€ë¶„ì´ ì—ëŸ¬ê°€ ë‚˜ì„œ `class Embedding: ` ì— np.add.at ëŒ€ì‹  np.scatter_addë¥¼ ì ì–´ì£¼ì—ˆìœ¼ë‹ˆ í˜¹ì‹œ CPUë¡œ ëŒë ¤ì•¼í•œë‹¤ë©´ ì•„ë˜ì˜ ì½”ë“œì—ì„œ Embedding í´ë˜ìŠ¤ì˜ np.scatter_addë¥¼ np.add.atìœ¼ë¡œ ë°”ê¾¸ê³ , GPUë¥¼ falseë¡œ ë°”ê¾¼ë’¤ ì‹¤í–‰í•˜ê¸° ë°”ë€ë‹¤. ê·¸ë¦¬ê³  ì•„ë˜ ì½”ë“œë¥¼ ë³´ë©´ `cupy`ë¥¼ ì´ìš©í•´ GPU ì—°ì‚°ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ cupyë¥¼ ì„¤ì¹˜í•´ì•¼ì£¼ì–´ì•¼ í•œë‹¤. ê·¸ ì „ì— ìì‹ ì˜ ì„¤ì¹˜í™˜ê²½ì— ë§ëŠ” cudaì™€ ê·¸ì— ë§ëŠ” cupyë¥¼ ì„¤ì¹˜í•˜ëŠ” ê²ƒì´ í•µì‹¬! ìì„¸í•œ ê²ƒì€ [cupy document](<https://docs-cupy.chainer.org/en/stable/>)ë¥¼ ë³´ë©´ì„œ ì„¤ì¹˜í–ˆìœ¼ë‹ˆ, ì°¸ê³ í•˜ì‹œê¸¸!




```python
GPU = True
if GPU: # GPU
    import cupy as np
    np.cuda.set_allocator(np.cuda.MemoryPool().malloc)
    #np.add.at = np.scatter_add

    print('\033[92m' + '-' * 60 + '\033[0m')
    print(' ' * 23 + '\033[92mGPU Mode (cupy)\033[0m')
    print('\033[92m' + '-' * 60 + '\033[0m\n')
else :
    import numpy as np
```

    [92m------------------------------------------------------------[0m
                           [92mGPU Mode (cupy)[0m
    [92m------------------------------------------------------------[0m



## ëª©ì°¨

___



4.1 word2vec ê°œì„  1
- 4.1.1 Embedding ê³„ì¸µ
- 4.1.2 Embedding ê³„ì¸µ êµ¬í˜„

4.2 word2vec ê°œì„  2
- 4.2.1 ì€ë‹‰ì¸µ ì´í›„ ê³„ì‚°ì˜ ë¬¸ì œì 
- 4.2.2 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ
- 4.2.3 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- 4.2.4 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ êµ¬í˜„
- 4.2.5 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§
- 4.2.6 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ ìƒ˜í”Œë§ ê¸°ë²•
- 4.2.7 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ êµ¬í˜„

4.3 ê°œì„ íŒ word2vec í•™ìŠµ
- 4.3.1 CBOW ëª¨ë¸ êµ¬í˜„
- 4.3.2 CBOW ëª¨ë¸ í•™ìŠµ ì½”ë“œ
- 4.3.3 CBOW ëª¨ë¸ í‰ê°€

4.4 word2vec ë‚¨ì€ ì£¼ì œ
- 4.4.1 word2vecì„ ì‚¬ìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ì˜ˆ
- 4.4.2 ë‹¨ì–´ ë²¡í„° í‰ê°€ ë°©ë²•

4.5 ì •ë¦¬



# 4.1 word2vec ê°œì„  1

___



![p4-2](https://user-images.githubusercontent.com/24144491/59848681-b60f0980-93a0-11e9-987b-6fc8d97f5554.png)

ì§€ë‚œ ì‹œê°„ ì‚´í´ë³¸ word2vecì€ ì—¬ì „íˆ í° 2ê°€ì§€ ë¬¸ì œë¥¼ ì•ˆê³ ìˆë‹¤. ì˜ˆë¥¼ë“¤ì–´ ì–´íœ˜ìˆ˜ê°€ 100ë§Œê°œë¼ê³  í•œë‹¤ë©´ ë‹¤ìŒì˜ ê³¼ì •ì—ì„œ ìƒë‹¹í•œ ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ëŸ‰ì´ í•„ìš”í•˜ë‹¤.
- [1] ì…ë ¥ì¸µì˜ ì›í•« í‘œí˜„ê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ W_in ì˜ ê³± ê³„ì‚° : 4.1 ì„ë² ë”© ê³„ì¸µ êµ¬í˜„ìœ¼ë¡œ í•´ê²°
- [2] ì€ë‹‰ì¸µê³¼ ê°€ì¤‘ì¹˜ í–‰ë ¬ W_outì˜ ê³± ë° Softmax ê³„ì¸µì˜ ê³„ì‚° : 4.2 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ìœ¼ë¡œ í•´ê²°

4.1ì—ì„œëŠ” ë¨¼ì € **Embedding** ê³„ì¸µ êµ¬í˜„ë¶€í„° ì‚´í´ë³¸ë‹¤.



### 4.1.1 Embedding ê³„ì¸µ
![p4-1-1](https://user-images.githubusercontent.com/24144491/59848679-b60f0980-93a0-11e9-9d19-238ce3a8b238.png)

> corrections : hidden layer shape = (1 x 100)

: ì‹¤ì œ ë‹¨ì–´ì˜ ì›í•« í‘œí˜„ê³¼ W_inì˜ ê°€ì¤‘ì¹˜ ë§¤íŠ¸ë¦­ìŠ¤ê°„ì˜ ê³±ì˜ ê²°ê³¼ëŠ” W_in[ì›í•« í‘œí˜„ì—ì„œ í•´ë‹¹ ë‹¨ì–´ì˜ index ë²ˆì§¸ í–‰] ê³¼ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.



### 4.1.2 Embedding ê³„ì¸µ êµ¬í˜„
- `class Embedding`
  : **forward** ë•ŒëŠ” indexë§Œ ë„˜ê²¨ ì€ë‹‰ì¸µ(h)ë¥¼ ì–»ê³ 
  ![p4-1-2](https://user-images.githubusercontent.com/24144491/59848680-b60f0980-93a0-11e9-90f3-42881ffd9c78.png)

  : **backward** ë•ŒëŠ” dW ì—…ë°ì´íŠ¸ í•´ì•¼í•  indexê°€ ê²¹ì¹˜ëŠ” ê²½ìš°ê°€ ìˆì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ 'í• ë‹¹'ì´ ì•„ë‹Œ 'ë”í•˜ê¸°'ë¥¼ í•´ì•¼ í•œë‹¤.

    

```python
# 4.1.2 Embedding ê³„ì¸µì„ êµ¬í˜„ì„ ì´í•´í•˜ê¸° ìœ„í•œ ê¸°ë³¸ì ì¸ í–‰ë ¬ ì—°ì‚°ë“¤
#import numpy as np
W = np.arange(21).reshape(7,3)
W
```




    array([[ 0,  1,  2],
           [ 3,  4,  5],
           [ 6,  7,  8],
           [ 9, 10, 11],
           [12, 13, 14],
           [15, 16, 17],
           [18, 19, 20]])




```python
print("W[2] : ", W[2])
print("W[5] : ",W[5])
idx = np.array([1,0,3,0])
print("W[[1,0,3,0]] = W[idx] : \n",W[idx])
```

    W[2] :  [6 7 8]
    W[5] :  [15 16 17]
    W[[1,0,3,0]] = W[idx] : 
     [[ 3  4  5]
     [ 0  1  2]
     [ 9 10 11]
     [ 0  1  2]]



```python
# 4.1.2 Embedding ê³„ì¸µ êµ¬í˜„
class Embedding:
    def __init__(self, W):
        self.params = [W]
        self.grads = [np.zeros_like(W)]
        self.idx = None

    def forward(self, idx):
        W, = self.params
        self.idx = idx
        out = W[idx] 
        return out

    def backward(self, dout):
        dW, = self.grads
        dW[...] = 0
        np.scatter_add(dW, self.idx, dout) # cpuëŠ” numpy ì¼ë•Œ, np.add.at(dW, self.idx, dout)
        return None
  
```

>  **[Warning]** ê°€ì¤‘ì¹˜ Wì™€ í¬ê¸°ê°€ ê°™ì€ í–‰ë ¬ dWë¥¼ ë§Œë“¤ê³  Wì— ë”í•´ì£¼ëŠ” ì‹ìœ¼ë¡œ ì ìš©í•˜ë ¤ê³  backwardë¥¼ dW[...]=0ìœ¼ë¡œ êµ¬í˜„í•¨ (Optimizer) í´ë˜ìŠ¤ì™€ ì¡°í•©í•´ ì‚¬ìš©í•˜ê³ ì. í•˜ì§€ë§Œ ë¹„íš¨ìœ¨ì .ë°”ë¡œ Wì— doutì„ ë¹¼ì£¼ë©´ ë˜ë‹ˆê¹Œ. # Optimizer ë¶€ë¶„ ë´ì„œ ìˆ˜ì •í•´ë³¼ ê²ƒ.



# 4.2 word2vec ê°œì„  2

___



ì€ë‹‰ì¸µê¹Œì§€ì˜ ê³„ì‚°ì€ indexë¥¼ í™œìš©í•´ í•„ìš”í•œ ê³„ì‚°ë§Œ ì‹¤ì‹œí•˜ë¯€ë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë°”ê¿¨ë‹¤ê³  í•  ìˆ˜ ìˆìœ¼ë‚˜, ì—¬ì „íˆ W_out ë¶€ë¶„ì˜ ê³„ì‚°ê³¼ ìˆ˜ ë§ì€ ì–´íœ˜ì™€ Lossë¥¼ êµ¬í•´ì•¼ í•˜ëŠ” ìƒí™©. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ `ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§`ì„ ì´ìš©í•˜ê³ ì í•œë‹¤.



### 4.2.1 ì€ë‹‰ì¸µ ì´í›„ ê³„ì‚°ì˜ ë¬¸ì œì 
- ì€ë‹‰ì¸µê³¼ W_out (ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ìˆ˜ x ì–´íœ˜ìˆ˜)ê°„ì˜ í–‰ë ¬ê³±ì€ ì—¬ì „íˆ í° ê³„ì‚°ëŸ‰
- Softmax ê³„ì¸µ ê³„ì‚° ì—­ì‹œ ë™ì¼ : kë²ˆì§¸ ì›ì†Œë¥¼ targetìœ¼ë¡œ í–ˆì„ë•Œ ìš°ë¦¬ê°€ êµ¬í•´ì•¼í•˜ëŠ” ê°’ y_k = exp(s_k) / sum(exp^(s_i)) : ê²°êµ­ ì–´íœ˜ìˆ˜(ì˜ˆë¥¼ë“¤ì–´ 100ë§Œê°œë¼ë©´ ë°±ë§Œë²ˆ)ë§Œí¼ ê³„ì‚° í•„ìš”.

### 4.2.2 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ
- ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ ê·¼ì‚¬í•˜ëŠ” ê²ƒì´ ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì„ ì´í•´í•˜ëŠ” ë° ì¤‘ìš”í•œ ê°œë…
- ì¦‰ target ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” indexì˜ ê°’ë§Œ í™•ë¥ ë¡œ êµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œ
  ![p4-2-2](https://user-images.githubusercontent.com/24144491/59848682-b60f0980-93a0-11e9-89a3-6d10f18e4059.png)

### 4.2.3 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì™€ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨
- ë§ˆì§€ë§‰ ì¶œë ¥ì¸µ ê°’ì„ í™•ë¥ ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ `sigmoid` í•¨ìˆ˜ í™œìš© : y = 1 / (1 + exp(-x))
- ![s4-3](https://user-images.githubusercontent.com/24144491/59848684-b6a7a000-93a0-11e9-9760-a1d761ca31e0.png)

- [warning] ì±…ì—ì„œëŠ” [ì‹ 1.7] L = -(ì‹œê·¸ë§ˆ_k(t_k x log(y_k)) ì™€ ìœ„ì˜ [ì‹ 4.3]ì´ ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì¶œë ¥ì¸µì— ë‰´ëŸ°ì„ 2ê°œë§Œ ì‚¬ìš©í•  ê²½ìš° ìœ„ì˜ ì‹ê³¼ ê°™ì•„ì§„ë‹¤

- ![s4-4](https://user-images.githubusercontent.com/24144491/59848678-b5767300-93a0-11e9-934e-217c34371bd3.png)

> corrections : delta(L) / delta(y)  = - (t/y) + (1-t)/(1-y) = (y-t)/(y(1-y))

  : ì¶œë ¥ì¸µì˜ backwardì˜ ê²½ìš° Lì„ x(sigmoid ì „)ë¡œ ë¯¸ë¶„í•œ ê°’, *y-t* ë§Œ ë„˜ê²¨ì£¼ë©´ ëœë‹¤. (ë§¤ìš° simple!)



### 4.2.4 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ êµ¬í˜„
- `class Embedding Dot`



### 4.2.5 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§

- 4.2.4 ê¹Œì§€ëŠ” target, ì¦‰ ì •ë‹µì¸ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” Lossë§Œ êµ¬í•˜ê²Œ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì •ë‹µì´ ì•„ë‹Œ ë‹¤ë¥¸ ë‹¨ì–´ì— ëŒ€í•œ í™•ë¥ ê°’ì€ ì–´ë–»ê²Œ êµ¬í• ì§€ ì˜ í•™ìŠµí•˜ì§€ ëª»í•œë‹¤.
- ì •ë‹µì´ ì•„ë‹Œ ë‹¨ì–´ëŠ” ë‚®ì€ í™•ë¥ ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆê²Œ í•™ìŠµí•˜ë„ë¡ ë¶€ì •ì ì¸ ì˜ˆ, negative sample ëª‡ ê°€ì§€ë¥¼ ë” ë„£ì–´ì£¼ì.
- ì˜ˆë¥¼ ë“¤ì–´, you say goodbye and i hello . ì—ì„œ you ì™€ goodbyeê°€ inputìœ¼ë¡œ ë“¤ì–´ê°”ë‹¤ë©´, ê·¸ì— ëŒ€í•œ ë‹µì€ [0, 1, 0, 0, ..  ,0] ì´ ë˜ì–´ì•¼ í•œë‹¤. ë”°ë¼ì„œ target index, sayì˜ indexëŠ” 1ë¡œ ì˜ˆì¸¡í•´ì•¼í•˜ê³ , ë‚˜ë¨¸ì§€ indexëŠ” 0ìœ¼ë¡œ ì˜ˆì¸¡í•´ì•¼í•œë‹¤ëŠ” ëœ». target = [0,1,0,..,0] ì—ì„œ ì •ë‹µ indexëŠ” 1ì´ê³  ë‚˜ë¨¸ì§€ 0,2,3,4,5,...,vocab_size ëŠ” ëª¨ë‘ 0ìœ¼ë¡œ ì˜ˆì¸¡í•´ì•¼í•œë‹¤. ì´ë•Œ, 3,4,5 ë“±ì„ ë¶€ì •ì ì¸ ì˜ˆë¡œ ì¶”ê°€í•´ ì¶œë ¥ì¸µì„ êµ¬ì„±í•˜ê³  ê·¸ ì¶œë ¥ì¸µì— ëŒ€í•œ lossë¥¼ ê³„ì‚°í•˜ê³  ì—­ì „íŒŒí•œë‹¤ë©´ ëª‡ ê°€ì§€ ë¶€ì •ì ì¸ ì˜ˆì‹œì— ëŒ€í•´ì„œë„ ì˜ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. 

### 4.2.6 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ì˜ ìƒ˜í”Œë§ ê¸°ë²•
- ê·¸ë ‡ë‹¤ë©´ ë¶€ì •ì ì¸ ì˜ˆë¥¼ ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ sampling í•  ê²ƒì¸ê°€?
- ê·¸ì— ëŒ€í•œ ë‹µì€, ë§ë­‰ì¹˜ì˜ ë‹¨ì–´ë³„ ì¶œí˜„ íšŸìˆ˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ í™•ë¥  ë¶„í¬ë¥¼ êµ¬í•œë‹¤. : `UnigramSampler(corpus, power, sample_size)`
- ì´ë•Œ, ì¶œí˜„ ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´ì˜ ì„ íƒì„ ë†’ì—¬ì£¼ê¸° ìœ„í•´ í™•ë¥  ë¶„í¬ì—ì„œ êµ¬í•œ ê°’ë“¤ 0.75 ì œê³±í•˜ê³  í•´ë‹¹ í™•ë¥  ê°’ì„ ë‹¤ì‹œ êµ¬í•œë‹¤. ì¦‰, ì¶œí˜„ ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´ì˜ í™•ë¥  ê°’ì„ ë†’ì—¬ì£¼ê³ , ë‹¤ë¥¸ í™•ë¥  ê°’ì€ ìƒëŒ€ì ìœ¼ë¡œ ë‚®ì¶œ ìˆ˜ ìˆê²Œ ë˜ì–´ ë¹„êµì  ê³¨ê³ ë£¨ ë‹¨ì–´ê°€ ì„ íƒë˜ë„ë¡ í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.



### 4.2.7 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ êµ¬í˜„

- `class NagetiveSamplingLoss`

```python

import collections
# 4.2.3 sigmoid with loss êµ¬í˜„
def softmax(x):
    if x.ndim == 2:
        x = x - x.max(axis=1, keepdims=True)
        x = np.exp(x)
        x /= x.sum(axis=1, keepdims=True)
    elif x.ndim == 1:
        x = x - np.max(x)
        x = np.exp(x) / np.sum(np.exp(x))

    return x

def cross_entropy_error(y, t):
    if y.ndim == 1:
        t = t.reshape(1, t.size)
        y = y.reshape(1, y.size)
        
    # ì •ë‹µ ë°ì´í„°ê°€ ì›í•« ë²¡í„°ì¼ ê²½ìš° ì •ë‹µ ë ˆì´ë¸” ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    if t.size == y.size:
        t = t.argmax(axis=1)
             
    batch_size = y.shape[0]

    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size

class SigmoidWithLoss:
    def __init__(self):
        self.params, self.grads = [], []
        self.loss = None
        self.y = None  #  # sigmoidì˜ ì¶œë ¥
        self.t = None  # ì •ë‹µ ë°ì´í„°

    def forward(self, x, t):
        self.t = t
        self.y = 1 / (1 + np.exp(-x))

        self.loss = cross_entropy_error(np.c_[1 - self.y, self.y], self.t)

        return self.loss

    def backward(self, dout=1):
        batch_size = self.t.shape[0]

        dx = (self.y - self.t) * dout / batch_size
        return dx

# 4.2.4 ë‹¤ì¤‘ ë¶„ë¥˜ì—ì„œ ì´ì§„ ë¶„ë¥˜ë¡œ êµ¬í˜„
class EmbeddingDot:
    def __init__(self, W):
        self.embed = Embedding(W)
        self.params = self.embed.params
        self.grads = self.embed.grads
        self.cache = None

    def forward(self, h, idx):
        target_W = self.embed.forward(idx)
        out = np.sum(target_W * h, axis=1)

        self.cache = (h, target_W)
        return out

    def backward(self, dout):
        h, target_W = self.cache
        dout = dout.reshape(dout.shape[0], 1)

        dtarget_W = dout * h
        self.embed.backward(dtarget_W)
        dh = dout * target_W
        return dh

# 4.2.7 ë„¤ê±°í‹°ë¸Œ ìƒ˜í”Œë§ êµ¬í˜„
class UnigramSampler:
    def __init__(self, corpus, power, sample_size):
        self.sample_size = sample_size
        self.vocab_size = None
        self.word_p = None

        counts = collections.Counter()
        for word_id in corpus:
            counts[word_id] += 1

        vocab_size = len(counts)
        self.vocab_size = vocab_size

        self.word_p = np.zeros(vocab_size)
        for i in range(vocab_size):
            self.word_p[i] = counts[i]

        self.word_p = np.power(self.word_p, power)
        self.word_p /= np.sum(self.word_p)

    def get_negative_sample(self, target):
        batch_size = target.shape[0]

        if not GPU:
            negative_sample = np.zeros((batch_size, self.sample_size), dtype=np.int32)

            for i in range(batch_size):
                p = self.word_p.copy()
                target_idx = np.asnumpy(target[i])
                p[target_idx] = 0
                p /= p.sum()
                negative_sample[i, :] = np.random.choice(self.vocab_size, size=self.sample_size, replace=False, p=p)
        else:
            # GPU(cupyï¼‰ë¡œ ê³„ì‚°í•  ë•ŒëŠ” ì†ë„ë¥¼ ìš°ì„ í•œë‹¤.
            # ë¶€ì •ì  ì˜ˆì— íƒ€ê¹ƒì´ í¬í•¨ë  ìˆ˜ ìˆë‹¤.
            negative_sample = np.random.choice(self.vocab_size, size=(batch_size, self.sample_size),
                                               replace=True, p=self.word_p)

        return negative_sample


class NegativeSamplingLoss:
    def __init__(self, W, corpus, power=0.75, sample_size=5):
        self.sample_size = sample_size
        self.sampler = UnigramSampler(corpus, power, sample_size)
        self.loss_layers = [SigmoidWithLoss() for _ in range(sample_size + 1)]
        self.embed_dot_layers = [EmbeddingDot(W) for _ in range(sample_size + 1)]

        self.params, self.grads = [], []
        for layer in self.embed_dot_layers:
            self.params += layer.params
            self.grads += layer.grads

    def forward(self, h, target):
        batch_size = target.shape[0]
        negative_sample = self.sampler.get_negative_sample(target)

        # ê¸ì •ì  ì˜ˆ ìˆœì „íŒŒ
        score = self.embed_dot_layers[0].forward(h, target)
        correct_label = np.ones(batch_size, dtype=np.int32)
        loss = self.loss_layers[0].forward(score, correct_label)

        # ë¶€ì •ì  ì˜ˆ ìˆœì „íŒŒ
        negative_label = np.zeros(batch_size, dtype=np.int32)
        for i in range(self.sample_size):
            negative_target = negative_sample[:, i]
            score = self.embed_dot_layers[1 + i].forward(h, negative_target)
            loss += self.loss_layers[1 + i].forward(score, negative_label)

        return loss

    def backward(self, dout=1):
        dh = 0
        for l0, l1 in zip(self.loss_layers, self.embed_dot_layers):
            dscore = l0.backward(dout)
            dh += l1.backward(dscore)

        return dh

```

# 4.3 ê°œì„ íŒ word2vec í•™ìŠµ

___



![p4-2-4](https://user-images.githubusercontent.com/24144491/59848683-b6a7a000-93a0-11e9-8b16-05a7180824f4.png)

- 4.3.1 CBOW ëª¨ë¸ êµ¬í˜„
- 4.3.2 CBOW ëª¨ë¸ í•™ìŠµ ì½”ë“œ
- 4.3.3 CBOW ëª¨ë¸ í‰ê°€


```python
# 4.3.1 CBOW ëª¨ë¸ êµ¬í˜„
import sys
sys.path.append('D:/ANACONDA/envs/tf-gpu/code/NLP')

class CBOW:
    def __init__(self, vocab_size,hidden_size,window_size, corpus):
        V, H = vocab_size, hidden_size
        
        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        W_in = 0.01 * np.random.randn(V,H).astype('f')
        W_out = 0.01 * np.random.rand(V,H).astype('f')
        
        # ê³„ì¸µ ìƒì„±
        self.in_layers = []
        for i in range(2 * window_size):
            layer = Embedding(W_in)
            self.in_layers.append(layer)
        self.ns_loss = NegativeSamplingLoss(W_out,corpus, power=0.75, sample_size=5)
        
        # ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë°°ì—´ì— ëª¨ì€ë‹¤.
        layers = self.in_layers + [self.ns_loss]
        self.params, self.grads = [],[]
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads
        
        #ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ ì €ì¥í•œë‹¤.
        self.word_vecs = W_in
    
    def forward(self, contexts, target):
        h = 0
        for i, layer in enumerate(self.in_layers):
            h += layer.forward(contexts[:,i])
        h *= 1/ len(self.in_layers)
        loss = self.ns_loss.forward(h,target)
        return loss
    
    def backward(self, dout = 1):
        dout = self.ns_loss.backward(dout)
        dout *= 1 / len(self.in_layers)
        for layer in self.in_layers:
            layer.backward(dout)
        return None
    
# Skip-Gram
class SkipGram:
    def __init__(self, vocab_size, hidden_size, window_size, corpus):
        V, H = vocab_size, hidden_size
        rn = np.random.randn

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        W_in = 0.01 * rn(V, H).astype('f')
        W_out = 0.01 * rn(V, H).astype('f')

        # ê³„ì¸µ ìƒì„±
        self.in_layer = Embedding(W_in)
        self.loss_layers = []
        for i in range(2 * window_size):
            layer = NegativeSamplingLoss(W_out, corpus, power=0.75, sample_size=5)
            self.loss_layers.append(layer)

        # ëª¨ë“  ê°€ì¤‘ì¹˜ì™€ ê¸°ìš¸ê¸°ë¥¼ ë¦¬ìŠ¤íŠ¸ì— ëª¨ì€ë‹¤.
        layers = [self.in_layer] + self.loss_layers
        self.params, self.grads = [], []
        for layer in layers:
            self.params += layer.params
            self.grads += layer.grads

        # ì¸ìŠ¤í„´ìŠ¤ ë³€ìˆ˜ì— ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ ì €ì¥í•œë‹¤.
        self.word_vecs = W_in

    def forward(self, contexts, target):
        h = self.in_layer.forward(target)

        loss = 0
        for i, layer in enumerate(self.loss_layers):
            loss += layer.forward(h, contexts[:, i])
        return loss

    def backward(self, dout=1):
        dh = 0
        for i, layer in enumerate(self.loss_layers):
            dh += layer.backward(dout)
        self.in_layer.backward(dh)
        return None
```


```python
# í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜
import numpy
import time
import matplotlib.pyplot as plt
#from common.np import *  # import numpy as np
#from common.util import clip_grads
def clip_grads(grads, max_norm):
    total_norm = 0
    for grad in grads:
        total_norm += np.sum(grad ** 2)
    total_norm = np.sqrt(total_norm)

    rate = max_norm / (total_norm + 1e-6)
    if rate < 1:
        for grad in grads:
            grad *= rate
class Trainer:
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer
        self.loss_list = []
        self.eval_interval = None
        self.current_epoch = 0

    def fit(self, x, t, max_epoch=10, batch_size=32, max_grad=None, eval_interval=500):
        data_size = len(x)
        max_iters = data_size // batch_size
        self.eval_interval = eval_interval
        model, optimizer = self.model, self.optimizer
        total_loss = 0
        loss_count = 0

        start_time = time.time()
        for epoch in range(max_epoch):
            # ë’¤ì„ê¸°Â¸Â°
            idx = numpy.random.permutation(numpy.arange(data_size))
            x = x[idx]
            t = t[idx]

            for iters in range(max_iters):
                batch_x = x[iters*batch_size:(iters+1)*batch_size]
                batch_t = t[iters*batch_size:(iters+1)*batch_size]

                # ê¸°ìš¸ê¸° êµ¬í•´ ë§¤ê°œë³€ìˆ˜ ê°±ì‹ 
                loss = model.forward(batch_x, batch_t)
                model.backward()
                params, grads = remove_duplicate(model.params, model.grads)  # Ä™Å‚Ä¾Ä›ÂœÂ Ã«ÂÂœ Ä™Â°Â€Ä›Â¤Â‘Ä›Å¡Â˜Ã«Ä½Åº Ã­Â•Â˜Ã«Â‚Â˜Ã«Ä„Âœ Ã«ÅÂ¨Ä›ÂÂŒ
                if max_grad is not None:
                    clip_grads(grads, max_grad)
                optimizer.update(params, grads)
                total_loss += loss
                loss_count += 1

                # í‰ê°€
                if (eval_interval is not None) and (iters % eval_interval) == 0:
                    avg_loss = total_loss / loss_count
                    elapsed_time = time.time() - start_time
                    
                    print('| ì—í­ %d |  ë°˜ë³µ %d / %d | ì‹œê°„ %d[s] | ì†ì‹¤ %.2f'
                          % (self.current_epoch + 1, iters + 1, max_iters, elapsed_time, avg_loss))
                    self.loss_list.append(float(avg_loss))
                    total_loss, loss_count = 0, 0

            self.current_epoch += 1

    def plot(self, ylim=None):
        x = numpy.arange(len(self.loss_list))
        if ylim is not None:
            plt.ylim(*ylim)
        plt.plot(x, self.loss_list, label='train')
        plt.xlabel('ë°˜ë³µ (x'  + str(self.eval_interval) + ')')
        plt.ylabel('ì†ì‹¤')
        plt.show()

#import sys
#sys.path.append('..')

def remove_duplicate(params, grads):
    '''
    ë§¤ê°œë³€ìˆ˜ ë°°ì—´ ì¤‘ ì¤‘ë³µë˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ í•˜ë‚˜ë¡œ ëª¨ì•„
    ê·¸ ê°€ì¤‘ì¹˜ì— ëŒ€ì‘í•˜ëŠ” ê¸°ìš¸ê¸°ë¥¼ ë”í•œë‹¤.
    '''
    params, grads = params[:], grads[:]  # copy list

    while True:
        find_flg = False
        L = len(params)

        for i in range(0, L - 1):
            for j in range(i + 1, L):
                # ê°€ì¤‘ì¹˜ ê³µìœ  ì‹œ
                if params[i] is params[j]:
                    grads[i] += grads[j]  
                    find_flg = True
                    params.pop(j)
                    grads.pop(j)
                # ê°€ì¤‘ì¹˜ë¥¼ ì „ì¹˜í–‰ë ¬ë¡œ ê³µìœ í•˜ëŠ” ê²½ìš°(weight tying)
                elif params[i].ndim == 2 and params[j].ndim == 2 and \
                     params[i].T.shape == params[j].shape and np.all(params[i].T == params[j]):
                    grads[i] += grads[j].T
                    find_flg = True
                    params.pop(j)
                    grads.pop(j)

                if find_flg: break
            if find_flg: break

        if not find_flg: break

    return params, grads
import pickle
#from common.trainer import Trainer
#from common.optimizer import Adam
class Adam:
    '''
    Adam (http://arxiv.org/abs/1412.6980v8)
    '''
    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        self.lr = lr
        self.beta1 = beta1
        self.beta2 = beta2
        self.iter = 0
        self.m = None
        self.v = None
        
    def update(self, params, grads):
        if self.m is None:
            self.m, self.v = [], []
            for param in params:
                self.m.append(np.zeros_like(param))
                self.v.append(np.zeros_like(param))
        
        self.iter += 1
        lr_t = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)

        for i in range(len(params)):
            self.m[i] += (1 - self.beta1) * (grads[i] - self.m[i])
            self.v[i] += (1 - self.beta2) * (grads[i]**2 - self.v[i])
            
            params[i] -= lr_t * self.m[i] / (np.sqrt(self.v[i]) + 1e-7)

#from cbow import CBOW
#from common.util import create_contexts_target, to_cpu, to_gpu
def create_contexts_target(corpus, window_size=1):
    '''ë§¥ë½ê³¼ íƒ€ê¹ƒ ìƒì„±

    :param corpus: ë§ë­‰ì¹˜(ë‹¨ì–´ ID ëª©ë¡)
    :param window_size: ìœˆë„ìš° í¬ê¸°(ìœˆë„ìš° í¬ê¸°ê°€ 1ì´ë©´ íƒ€ê¹ƒ ë‹¨ì–´ ì¢Œìš° í•œ ë‹¨ì–´ì”©ì´ ë§¥ë½ì— í¬í•¨)
    :return:
    '''
    target = corpus[window_size:-window_size]
    contexts = []

    for idx in range(window_size, len(corpus)-window_size):
        cs = []
        for t in range(-window_size, window_size + 1):
            if t == 0:
                continue
            cs.append(corpus[idx + t])
        contexts.append(cs)

    return np.array(contexts), np.array(target)


def to_cpu(x):
    import numpy
    if type(x) == numpy.ndarray:
        return x
    return np.asnumpy(x)


def to_gpu(x):
    import cupy
    if type(x) == cupy.ndarray:
        return x
    return cupy.asarray(x)

```


```python
# 4.3.2 í•™ìŠµ
from dataset import ptb

# í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì„¤ì •
window_size = 5
hidden_size = 100
batch_size = 100
max_epoch = 10

# ë°ì´í„° ì½ê¸° + target, contexts ë§Œë“¤ê¸°
corpus, word_to_id, id_to_word = ptb.load_data('train')
vocab_size = len(word_to_id)

contexts, target = create_contexts_target(corpus, window_size)
if GPU:
    contexts, target = to_gpu(contexts), to_gpu(target)
    
# ëª¨ë¸ ë“± ìƒì„± - CBOW or SkipGram
model = CBOW(vocab_size, hidden_size, window_size, corpus)
# model = SkipGram(vocab_size, hidden_size, window_size, corpus)
optimizer = Adam()
trainer = Trainer(model,optimizer)

# í•™ìŠµ ì‹œì‘
trainer.fit(contexts, target, max_epoch, batch_size, eval_interval = 3000) # eval_interval=500
trainer.plot()
```

    | ì—í­ 1 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 0[s] | ì†ì‹¤ 4.16
    | ì—í­ 1 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 26[s] | ì†ì‹¤ 2.64
    | ì—í­ 1 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 52[s] | ì†ì‹¤ 2.42
    | ì—í­ 1 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 78[s] | ì†ì‹¤ 2.31
    | ì—í­ 2 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 81[s] | ì†ì‹¤ 2.27
    | ì—í­ 2 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 107[s] | ì†ì‹¤ 2.20
    | ì—í­ 2 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 133[s] | ì†ì‹¤ 2.14
    | ì—í­ 2 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 159[s] | ì†ì‹¤ 2.09
    | ì—í­ 3 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 162[s] | ì†ì‹¤ 2.08
    | ì—í­ 3 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 188[s] | ì†ì‹¤ 2.00
    | ì—í­ 3 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 214[s] | ì†ì‹¤ 1.98
    | ì—í­ 3 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 240[s] | ì†ì‹¤ 1.95
    | ì—í­ 4 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 243[s] | ì†ì‹¤ 1.94
    | ì—í­ 4 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 269[s] | ì†ì‹¤ 1.87
    | ì—í­ 4 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 295[s] | ì†ì‹¤ 1.87
    | ì—í­ 4 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 321[s] | ì†ì‹¤ 1.86
    | ì—í­ 5 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 324[s] | ì†ì‹¤ 1.85
    | ì—í­ 5 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 350[s] | ì†ì‹¤ 1.78
    | ì—í­ 5 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 376[s] | ì†ì‹¤ 1.78
    | ì—í­ 5 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 402[s] | ì†ì‹¤ 1.77
    | ì—í­ 6 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 404[s] | ì†ì‹¤ 1.78
    | ì—í­ 6 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 430[s] | ì†ì‹¤ 1.70
    | ì—í­ 6 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 456[s] | ì†ì‹¤ 1.71
    | ì—í­ 6 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 483[s] | ì†ì‹¤ 1.71
    | ì—í­ 7 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 485[s] | ì†ì‹¤ 1.71
    | ì—í­ 7 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 511[s] | ì†ì‹¤ 1.63
    | ì—í­ 7 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 538[s] | ì†ì‹¤ 1.64
    | ì—í­ 7 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 564[s] | ì†ì‹¤ 1.65
    | ì—í­ 8 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 566[s] | ì†ì‹¤ 1.65
    | ì—í­ 8 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 592[s] | ì†ì‹¤ 1.58
    | ì—í­ 8 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 619[s] | ì†ì‹¤ 1.59
    | ì—í­ 8 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 646[s] | ì†ì‹¤ 1.59
    | ì—í­ 9 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 648[s] | ì†ì‹¤ 1.59
    | ì—í­ 9 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 675[s] | ì†ì‹¤ 1.53
    | ì—í­ 9 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 701[s] | ì†ì‹¤ 1.54
    | ì—í­ 9 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 727[s] | ì†ì‹¤ 1.55
    | ì—í­ 10 |  ë°˜ë³µ 1 / 9295 | ì‹œê°„ 730[s] | ì†ì‹¤ 1.56
    | ì—í­ 10 |  ë°˜ë³µ 3001 / 9295 | ì‹œê°„ 756[s] | ì†ì‹¤ 1.48
    | ì—í­ 10 |  ë°˜ë³µ 6001 / 9295 | ì‹œê°„ 782[s] | ì†ì‹¤ 1.49
    | ì—í­ 10 |  ë°˜ë³µ 9001 / 9295 | ì‹œê°„ 809[s] | ì†ì‹¤ 1.50



![output_13_1](https://user-images.githubusercontent.com/24144491/59848685-b7d8cd00-93a0-11e9-8e79-650b5cf8d61c.png)



```python
# word_vecs, ë¶„ì‚°í‘œí˜„ ì €ì¥
word_vecs = model.word_vecs
if GPU :
    word_vecs = to_cpu(word_vecs)
params = {}
params['word_vecs'] = word_vecs.astype(np.float16)
params['word_to_id'] = word_to_id
params['id_to_word'] = id_to_word
pkl_file = 'cbow_params1.pkl'
with open(pkl_file,'wb') as f:
    pickle.dump(params,f,-1);
```


```python
# 4.3.3 CBOW ëª¨ë¸ í‰ê°€
# GPU -> CPU, cupy -> numpy ë¡œ
#from common.util import most_similar, analogy
import numpy as np
def cos_similarity(x, y, eps=1e-8):
    '''ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì‚°ì¶œ

    :param x: ë²¡í„°
    :param y: ë²¡í„°
    :param eps: '0ìœ¼ë¡œ ë‚˜ëˆ„ê¸°'ë¥¼ ë°©ì§€í•˜ê¸° ìœ„í•œ ì‘ì€ ê°’
    :return:
    '''
    nx = x / (np.sqrt(np.sum(x ** 2)) + eps)
    ny = y / (np.sqrt(np.sum(y ** 2)) + eps)
    return np.dot(nx, ny)

def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):
    '''ìœ ì‚¬ ë‹¨ì–´ ê²€ìƒ‰

    :param query: ì¿¼ë¦¬(í…ìŠ¤íŠ¸)
    :param word_to_id: ë‹¨ì–´ì—ì„œ ë‹¨ì–´ IDë¡œ ë³€í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    :param id_to_word: ë‹¨ì–´ IDì—ì„œ ë‹¨ì–´ë¡œ ë³€í™˜í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
    :param word_matrix: ë‹¨ì–´ ë²¡í„°ë¥¼ ì •ë¦¬í•œ í–‰ë ¬. ê° í–‰ì— í•´ë‹¹ ë‹¨ì–´ ë²¡í„°ê°€ ì €ì¥ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•œë‹¤.
    :param top: ìƒìœ„ ëª‡ ê°œê¹Œì§€ ì¶œë ¥í•  ì§€ ì§€ì •
    '''
    if query not in word_to_id:
        print('%s(ì„)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' % query)
        return

    print('\n[query] ' + query)
    query_id = word_to_id[query]
    query_vec = word_matrix[query_id]

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    vocab_size = len(id_to_word)

    similarity = np.zeros(vocab_size)
    for i in range(vocab_size):
        similarity[i] = cos_similarity(word_matrix[i], query_vec)

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¶œë ¥
    count = 0
    for i in (-1 * similarity).argsort():
        if id_to_word[i] == query:
            continue
        print(' %s: %s' % (id_to_word[i], similarity[i]))

        count += 1
        if count >= top:
            return
        
        
def normalize(x):
    if x.ndim == 2:
        s = np.sqrt((x * x).sum(1))
        x /= s.reshape((s.shape[0], 1))
    elif x.ndim == 1:
        s = np.sqrt((x * x).sum())
        x /= s
    return x

def analogy(a, b, c, word_to_id, id_to_word, word_matrix, top=5, answer=None):
    for word in (a, b, c):
        if word not in word_to_id:
            print('%s(ì„)ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.' % word)
            return

    print('\n[analogy] ' + a + ':' + b + ' = ' + c + ':?')
    a_vec, b_vec, c_vec = word_matrix[word_to_id[a]], word_matrix[word_to_id[b]], word_matrix[word_to_id[c]]
    query_vec = b_vec - a_vec + c_vec
    query_vec = normalize(query_vec)

    similarity = np.dot(word_matrix, query_vec)

    if answer is not None:
        print("==>" + answer + ":" + str(np.dot(word_matrix[word_to_id[answer]], query_vec)))

    count = 0
    for i in (-1 * similarity).argsort():
        if np.isnan(similarity[i]):
            continue
        if id_to_word[i] in (a, b, c):
            continue
        print(' {0}: {1}'.format(id_to_word[i], similarity[i]))

        count += 1
        if count >= top:
            return


```


```python
pkl_file = './cbow_params1.pkl'
with open(pkl_file, 'rb') as f:
    params = pickle.load(f)
    
word_vecs = params['word_vecs']
word_to_id = params['word_to_id']
id_to_word = params['id_to_word']

# ê°€ì¥ ë¹„ìŠ·í•œ(most similar) ë‹¨ì–´ ë½‘ê¸°
querys = ['you', 'year', 'car', 'toyota']
for query in querys:
    most_similar(query, word_to_id, id_to_word, word_vecs, top=5)
```


    [query] you
     we: 0.7529296875
     i: 0.724609375
     your: 0.623046875
     someone: 0.60693359375
     anybody: 0.60595703125
    
    [query] year
     month: 0.84033203125
     week: 0.76953125
     spring: 0.75634765625
     summer: 0.73681640625
     decade: 0.7021484375
    
    [query] car
     window: 0.61376953125
     truck: 0.59765625
     luxury: 0.59326171875
     auto: 0.58984375
     cars: 0.5625
    
    [query] toyota
     nissan: 0.6650390625
     nec: 0.64697265625
     honda: 0.64501953125
     minicomputers: 0.630859375
     ibm: 0.6279296875



```python
# ìœ ì¶”(analogy) ì‘ì—…
print('-'*50)
analogy('king', 'man', 'queen',  word_to_id, id_to_word, word_vecs)
analogy('take', 'took', 'go',  word_to_id, id_to_word, word_vecs)
analogy('car', 'cars', 'child',  word_to_id, id_to_word, word_vecs)
analogy('good', 'better', 'bad',  word_to_id, id_to_word, word_vecs)
```

    --------------------------------------------------
    
    [analogy] king:man = queen:?
     a.m: 6.46875
     woman: 5.25390625
     father: 4.7265625
     daffynition: 4.70703125
     toxin: 4.61328125
    
    [analogy] take:took = go:?
     're: 4.33203125
     went: 4.1796875
     came: 4.17578125
     were: 3.966796875
     are: 3.89453125
    
    [analogy] car:cars = child:?
     a.m: 6.93359375
     rape: 5.96484375
     daffynition: 5.8046875
     children: 5.3125
     incest: 5.3046875
    
    [analogy] good:better = bad:?
     more: 5.72265625
     less: 5.453125
     rather: 5.37890625
     greater: 4.5703125
     faster: 4.2265625




# 4.5 ì •ë¦¬

___

- Embedding ê³„ì¸µì€ ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì„ ë‹´ê³  ìˆë‹¤
- word2vecì˜ ê°œì„ ì„ ìœ„í•´ ë‹¤ìŒ 2ê°€ì§€ ì‘ì—…ì„ ìˆ˜í–‰í–ˆë‹¤.
  - Embedding ê³„ì¸µì—ì„œ íŠ¹ì • ë‹¨ì–´ì˜ indexë§Œ ë½‘ì•„ ê³„ì‚°í•˜ë„ë¡
  - Negative samplingì„ í†µí•´ ë‹¤ì¤‘ ë¶„ë¥˜ë¥¼ ì´ì§„ ë¶„ë¥˜ë¡œ, ëª‡ ê°€ì§€ì˜ ë‹¨ì–´ë“¤ì˜ í™•ë¥ ê°’ê³¼ Lossë¥¼ ê³„ì‚°í•˜ë„ë¡
   - wod2vecì˜ Embedding, ë¶„ì‚° í‘œí˜„ì—ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ê°€ ë“¤ì–´ê°€ ìˆê³  ë¹„ìŠ·í•œ ë§¥ë½ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ëŠ” Embedding ê³µê°„ì—ì„œ ì„œë¡œ ê°€ê¹Œì´ ìœ„ì¹˜í•œë‹¤.
- word2vecì˜ Embedding, ë¶„ì‚° í‘œí˜„ì„ ì´ìš©í•˜ë©´ ìœ ì¶” ë¬¸ì œë¥¼ ë²¡í„°ì˜ ë§ì…ˆê³¼ ëº„ì…ˆ ë¬¸ì œë¡œ í’€ ìˆ˜ ìˆë‹¤.
- ì „ì´ í•™ìŠµ ì¸¡ë©´ì—ì„œ íŠ¹íˆ ì¤‘ìš”í•˜ë©°, ë‹¨ì–´ì˜ ë¶„ì‚° í‘œí˜„ì€ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì´ìš©í•  ìˆ˜ ìˆë‹¤.

- ë§ˆì§€ë§‰ìœ¼ë¡œ CBOW ë„ì‹í™”ë¥¼ ê·¸ë ¤ë³´ë©° ì •ë¦¬í•´ ë³´ì•˜ë‹¤.

![p4-5](https://user-images.githubusercontent.com/24144491/59949997-b2b67380-94af-11e9-8dba-de676b304c5f.png)





# 4.6 ë²ˆì™¸

___

- [1] ì‚¬ì‹¤ ìœ„ì˜ ê·¸ë¦¼ì€ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê¸° ìœ„í•´ì„œ ì €ë ‡ê²Œ ë‚˜íƒ€ëƒˆì§€ë§Œ, ì±…ì—ì„œ ì‹¤ì œë¡œ êµ¬í˜„ëœ ì½”ë“œëŠ” ìœ„ì˜ ê·¸ë¦¼ê³¼ëŠ” ë‹¤ë¥´ë‹¤. Class Embeddingì¸ W_inê³¼ W_outì„ ê°ê° [window_size * 2] ì™€ [Target + Sample_size]ë§Œí¼ ë§Œë“¤ê³  forwardì™€ backwardë¥¼ ì§„í–‰í•˜ê³  ì¤‘ë³µëœ ê°€ì¤‘ì¹˜ë¥¼ í•˜ë‚˜ë¡œ ë‹¤ì‹œ ëª¨ìœ¼ëŠ” ì‘ì—…ì„ `remove_duplicate`ë¡œ ì²˜ë¦¬í•œë‹¤. ì´ ê³¼ì •ì´ ë²ˆê±°ë¡­ë‹¤ê³  ëŠê»´ W_inì„ 1ê°œë¡œ í†µí•©í•˜ê³ , Targetê³¼ Sample_size ìš©ì„ ìœ„í•œ W_outì„ 2ê°œ ë§ˆë ¨í•´ Weightë¥¼ ì¤„ì—¬ í•™ìŠµì„ ì‹œë„í•´ ë³´ì•˜ë‹¤.
  - ìœ„ì˜ ê³¼ì •ì„ ìœ„í•´ì„œëŠ” ë‹¤ìŒ Classì˜ __init__ , forward ì™€ backward ë¶€ë¶„ì„ ê³ ì³ì•¼ í•œë‹¤
    - CBOW
    - NegativeSamplingLoss
    - Embedding
  - ê³ ì¹œ ì½”ë“œëŠ” [ch4_word2vec_faster_improved](https://github.com/Taeu/NLP/blob/master/ch4_word2vec_faster_improved.ipynb) ì— ì˜¬ë ¤ë‘ì—ˆë‹¤.
  - ì†ë„ëŠ” êµì¬ë³´ë‹¤ 2.3ë°° ë¹ ë¥´ì§€ë§Œ ì„±ëŠ¥ì€ ë” ëŠë¦¬ê²Œ ê°œì„ ë˜ë©° ê°™ì€ ì‹œê°„ì„ í•™ìŠµí–ˆì„ ë•Œ êµì¬ì˜ ì½”ë“œê°€ ë” ë†’ì€ accuracyë¥¼ ë³´ì˜€ë‹¤.
    - ê·¸ ì´ìœ ëŠ” ë‚´ê°€ ê¼¼ê¼¼í•˜ê²Œ ë³¸ë‹¤ê³  ë´¤ì§€ë§Œ ì•„ì£¼ ì„¸ì‹¬íˆ ë‹¤ë£¨ì§€ ëª»í–ˆì„ ê°€ëŠ¥ì„±ì´ í¬ê³ 
    - í˜¹ì€ W_in ì„ backward í•  ë•Œ ì—¬ëŸ¬ ë‰´ëŸ°ì— í•´ë‹¹í•  ìˆ˜ ìˆëŠ” weightë“¤ì„ len(self.in_layers)ë¡œ ë‚˜ëˆ ì£¼ë©´ì„œ ë” ë¯¸ì„¸í•˜ê²Œ ì¡°ì •í•´ì£¼ì–´ì„œì´ì§€ ì•Šì„ê¹Œ?
- [2] `Skip Gram`ëª¨ë¸ë¡œë„ ëŒë ¤ ë³´ì•˜ë‹¤. ì½”ë“œëŠ” [ch4_word2vec_TU](https://github.com/Taeu/NLP/blob/master/ch4_word2vec_TU.ipynb) ì— ì—…ë¡œë“œí–ˆë‹¤. 
  - CBOW ë³´ë‹¤ í•™ìŠµì´ í›¨ì”¬ ì˜¤ë˜ ê±¸ë¦°ë‹¤. (ì•½ 6ë°° ì°¨ì´)
  - ìœ ì¶” ë¬¸ì œì˜ ì •í™•ë„ëŠ” CBOWë¥¼ ëŒë ¸ì„ ë•Œ ë³´ë‹¤ ë” ë–¨ì–´ì¡Œë‹¤.
- [ch4_word2vec_faster](https://github.com/Taeu/NLP/blob/master/ch4_word2vec_faster.ipynb)

